{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sergei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'To see a World in a Grain of Sand \\\n",
    "And a Heaven in a Wild Flower, \\\n",
    "Hold Infinity in the palm of your hand \\\n",
    "And Eternity in an hour.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'see', 'a', 'World', 'in', 'a', 'Grain', 'of', 'Sand', 'And', 'a', 'Heaven', 'in', 'a', 'Wild', 'Flower', ',', 'Hold', 'Infinity', 'in', 'the', 'palm', 'of', 'your', 'hand', 'And', 'Eternity', 'in', 'an', 'hour', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sergei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "words = [x for x in word_tokenize(text) if x not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'see',\n",
       " 'World',\n",
       " 'Grain',\n",
       " 'Sand',\n",
       " 'And',\n",
       " 'Heaven',\n",
       " 'Wild',\n",
       " 'Flower',\n",
       " ',',\n",
       " 'Hold',\n",
       " 'Infinity',\n",
       " 'palm',\n",
       " 'hand',\n",
       " 'And',\n",
       " 'Eternity',\n",
       " 'hour',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'see', 'world', 'grain', 'sand', 'and', 'heaven', 'wild', 'flower', ',', 'hold', 'infin', 'palm', 'hand', 'and', 'etern', 'hour', '.']\n"
     ]
    }
   ],
   "source": [
    "print(list(map(ps.stem, words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Sergei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LazyCorpusLoader in module nltk.corpus.util object:\n",
      "\n",
      "treebank/combined = class LazyCorpusLoader(builtins.object)\n",
      " |  treebank/combined(name, reader_cls, *args, **kwargs)\n",
      " |  \n",
      " |  To see the API documentation for this lazily loaded corpus, first\n",
      " |  run corpus.ensure_loaded(), and then run help(this_corpus).\n",
      " |  \n",
      " |  LazyCorpusLoader is a proxy object which is used to stand in for a\n",
      " |  corpus object before the corpus is loaded.  This allows NLTK to\n",
      " |  create an object for each corpus, but defer the costs associated\n",
      " |  with loading those corpora until the first time that they're\n",
      " |  actually accessed.\n",
      " |  \n",
      " |  The first time this object is accessed in any way, it will load\n",
      " |  the corresponding corpus, and transform itself into that corpus\n",
      " |  (by modifying its own ``__class__`` and ``__dict__`` attributes).\n",
      " |  \n",
      " |  If the corpus can not be found, then accessing this object will\n",
      " |  raise an exception, displaying installation instructions for the\n",
      " |  NLTK data package.  Once they've properly installed the data\n",
      " |  package (or modified ``nltk.data.path`` to point to its location),\n",
      " |  they can then use the corpus object without restarting python.\n",
      " |  \n",
      " |  :param name: The name of the corpus\n",
      " |  :type name: str\n",
      " |  :param reader_cls: The specific CorpusReader class, e.g. PlaintextCorpusReader, WordListCorpusReader\n",
      " |  :type reader: nltk.corpus.reader.api.CorpusReader\n",
      " |  :param nltk_data_subdir: The subdirectory where the corpus is stored.\n",
      " |  :type nltk_data_subdir: str\n",
      " |  :param *args: Any other non-keywords arguments that `reader_cls` might need.\n",
      " |  :param *kargs: Any other keywords arguments that `reader_cls` might need.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, attr)\n",
      " |  \n",
      " |  __init__(self, name, reader_cls, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pic = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "#pic.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4c7a91ea1ef4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python37\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "#python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package spacy:\n",
      "\n",
      "NAME\n",
      "    spacy - # coding: utf8\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    _align\n",
      "    _ml\n",
      "    about\n",
      "    analysis\n",
      "    attrs\n",
      "    cli (package)\n",
      "    compat\n",
      "    data (package)\n",
      "    displacy (package)\n",
      "    errors\n",
      "    glossary\n",
      "    gold\n",
      "    kb\n",
      "    lang (package)\n",
      "    language\n",
      "    lemmatizer\n",
      "    lexeme\n",
      "    lookups\n",
      "    matcher (package)\n",
      "    ml (package)\n",
      "    morphology\n",
      "    parts_of_speech\n",
      "    pipeline (package)\n",
      "    scorer\n",
      "    strings\n",
      "    symbols\n",
      "    syntax (package)\n",
      "    tests (package)\n",
      "    tokenizer\n",
      "    tokens (package)\n",
      "    util\n",
      "    vectors\n",
      "    vocab\n",
      "\n",
      "FUNCTIONS\n",
      "    blank(name, **kwargs)\n",
      "    \n",
      "    info(model=None, markdown=False, silent=False)\n",
      "    \n",
      "    load(name, **overrides)\n",
      "\n",
      "DATA\n",
      "    Errors = <spacy.errors.add_codes.<locals>.ErrorsWithCodes object>\n",
      "    Warnings = <spacy.errors.add_codes.<locals>.ErrorsWithCodes object>\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "VERSION\n",
      "    2.2.4\n",
      "\n",
      "FILE\n",
      "    d:\\python37\\lib\\site-packages\\spacy\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "n_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train.data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cats = ['rec.sport.hockey', 'sci.crypt','comp.graphics','talk.politics.misc']\n",
    "n_train = fetch_20newsgroups(subset='train',categories=cats)\n",
    "\n",
    "print(len(n_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True)\n",
    "\n",
    "#sparce matrix\n",
    "vectors = vectorizer.fit_transform(n_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2244, 36820)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectors.todense()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.28587133, 0.04780443, 0.02094763, 0.06912555, 0.08912667,\n",
       "         0.10379202, 0.02993483, 0.03155774, 0.13058722, 0.06589882,\n",
       "         0.25785352, 0.12061764, 0.08869737, 0.08325699, 0.07397006,\n",
       "         0.0586384 , 0.14293567, 0.13300191, 0.02267493, 0.15443625,\n",
       "         0.27286638, 0.01781504, 0.05779538, 0.14293567, 0.09942515,\n",
       "         0.02750707, 0.03225783, 0.02925843, 0.0490427 , 0.07592973,\n",
       "         0.01785476, 0.03782875, 0.12061764, 0.10704009, 0.08389331,\n",
       "         0.03241922, 0.05715541, 0.02937951, 0.01835481, 0.03720528,\n",
       "         0.05285844, 0.03140394, 0.06699585, 0.14293567, 0.12456651,\n",
       "         0.13571229, 0.14794012, 0.28587133, 0.26117444, 0.1110154 ,\n",
       "         0.07649705, 0.01781504, 0.428807  , 0.14293567, 0.03772177,\n",
       "         0.08673054, 0.03427821, 0.04832461, 0.07790929, 0.04734058,\n",
       "         0.08264261, 0.14293567, 0.03343855, 0.09299369, 0.08912667,\n",
       "         0.09974649, 0.08746826]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[vec != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add stopwords and lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sergei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizator = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "   return ' '.join([lemmatizator.lemmatize(x) for x in word_tokenize(text.lower()) if x not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": young @ is.s.u-tokyo.ac.jp ( young shio hong ) subject : looking dr. bala r. vatti 's email address nntp-posting-host : rabbit-gw organization : dept . information science , univ . tokyo , japan . distribution : comp.graphics x-bytes : 660 line : 27 hi ! looking email address author '' generic solution polygon clipping '' , communication acm , july 1992 , vol . 35 , . 7. got information author follows mr. bala r. vatti lcec , 65 river road , hudson , n.h. 03051 email : vatti @ waynar.lcec.lockheed want get related detailed paper topic author . failed send email address . information appreciated . thank much . best regard . s. h. young kunii lab dept . information science faculty science university tokyo bunkyo-ku , hongo 7-3-1 113 tokyo , japan email : young @ is.s.u-tokyo.ac.jp\n"
     ]
    }
   ],
   "source": [
    "print(clean(n_train.data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(preprocessor=clean)\n",
    "vectors = vectorizer.fit_transform(n_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2244, 34505)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vectors = vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(v1,v2):\n",
    "    v1 = np.asarray(v1).squeeze()\n",
    "    v2 = np.asarray(v2).squeeze()\n",
    "    return np.dot(v1,v2)/(norm(v1)*norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014948363079775973"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity(d_vectors[0], d_vectors[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(d_vectors[0]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f106099748>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sims = [cos_similarity(d_vectors[26], d_vectors[i]) for i in range(50)]\n",
    "plt.plot(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics.classification as metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(d_vectors, n_train.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1795, 34505)\n",
      "(1795,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss='hinge' => SVM \n",
    "sgd = SGDClassifier()\n",
    "\n",
    "sgd.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.983561232942536\n"
     ]
    }
   ],
   "source": [
    "print(metric.balanced_accuracy_score(y_test, sgd.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.metrics.classification in sklearn.metrics:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics.classification - Metrics to assess performance on classification task given class prediction\n",
      "\n",
      "DESCRIPTION\n",
      "    Functions named as ``*_score`` return a scalar value to maximize: the higher\n",
      "    the better\n",
      "    \n",
      "    Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\n",
      "    the lower the better\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        jaccard_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equal\n",
      "        to the ``jaccard_score`` function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, and perfect performance scores 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        recall_score, roc_auc_score\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None)\n",
      "        Compute the Brier score.\n",
      "        The smaller the Brier score, the better, hence the naming with \"loss\".\n",
      "        Across all items in a set N predictions, the Brier score measures the\n",
      "        mean squared difference between (1) the predicted probability assigned\n",
      "        to the possible outcomes for item i, and (2) the actual outcome.\n",
      "        Therefore, the lower the Brier score is for a set of predictions, the\n",
      "        better the predictions are calibrated. Note that the Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1). The Brier loss is composed of refinement loss and\n",
      "        calibration loss.\n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter pos_label, which defaults to 1.\n",
      "        Read more in the :ref:`User Guide <calibration>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_prob : array, shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label of the positive class.\n",
      "            Defaults to the greater label unless y_true is all 0 or all -1\n",
      "            in which case pos_label defaults to 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob,                          pos_label=\"ham\")  # doctest: +ELLIPSIS\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score.\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_\n",
      "    \n",
      "    classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False)\n",
      "        Build a text report showing the main classification metrics\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_labels]\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : list of strings\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool (default = False)\n",
      "            If True, return output as dict\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : string / dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), sample average (only for multilabel classification) and\n",
      "            micro average (averaging the total true positives, false negatives and\n",
      "            false positives) it is only shown for multi-label or multi-class\n",
      "            with a subset of classes because it is accuracy otherwise.\n",
      "            See also:func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, confusion_matrix,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, labels=None, weights=None, sample_weight=None)\n",
      "        Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array, shape = [n_samples]\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array, shape = [n_samples]\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array, shape = [n_classes], optional\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If None, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : str, optional\n",
      "            List of weighting type to calculate the score. None means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               doi:10.1177/001316446002000104.\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596.\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa.\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` but\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_classes], optional\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If none is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : array, shape = [n_classes, n_classes]\n",
      "            Confusion matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure\n",
      "        \n",
      "        The F1 score can be interpreted as a weighted average of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is::\n",
      "        \n",
      "            F1 = 2 * (precision * recall) / (precision + recall)\n",
      "        \n",
      "        In the multi-class and multi-label case, this is the average of\n",
      "        the F1 score of each class with weighting depending on the ``average``\n",
      "        parameter.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        fbeta_score, precision_recall_fscore_support, jaccard_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the F-beta score\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter determines the weight of recall in the combined\n",
      "        score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n",
      "        favors recall (``beta -> 0`` considers only precision, ``beta -> inf``\n",
      "        only recall).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Weight of precision in harmonic mean.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array([0.71..., 0.        , 0.        ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, labels=None, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array, shape = [n_labels], optional (default='deprecated')\n",
      "            Integer array of labels. If not provided, labels will be inferred\n",
      "            from y_true and y_pred.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "            .. deprecated:: 0.21\n",
      "               This parameter ``labels`` is deprecated in version 0.21 and will\n",
      "               be removed in version 0.23. Hamming loss uses ``y_true.shape[1]``\n",
      "               for the number of labels when y_true is binary label indicators,\n",
      "               so it is unnecessary for the user to specify.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, jaccard_score, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized)\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array, shape = [n_samples] or [n_samples, n_classes]\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array, optional, default None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero.\n",
      "               <http://www.ttic.edu/sigml/symposium2011/papers/\n",
      "               Moore+DeNero_Regularization.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "        LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "             multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "             verbose=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision  # doctest: +ELLIPSIS\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)  # doctest: +NORMALIZE_WHITESPACE\n",
      "        LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "             intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "             multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "             verbose=0)\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS\n",
      "        0.56...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Jaccard similarity coefficient score\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float (if average is not None) or array of floats, shape =            [n_unique_labels]\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, f_score, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])  # doctest: +ELLIPSIS\n",
      "        0.6666...\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833...\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666...\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
      "        array([1. , 0. , 0.33...])\n",
      "    \n",
      "    jaccard_similarity_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Jaccard similarity coefficient score\n",
      "        \n",
      "        .. deprecated:: 0.21\n",
      "            This is deprecated to be removed in 0.23, since its handling of\n",
      "            binary and multiclass inputs was broken. `jaccard_score` has an API\n",
      "            that is consistent with precision_score, f_score, etc.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the sum of the Jaccard similarity coefficient\n",
      "            over the sample set. Otherwise, return the average of Jaccard\n",
      "            similarity coefficient.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the average Jaccard similarity\n",
      "            coefficient, else it returns the sum of the Jaccard similarity\n",
      "            coefficient over the sample set.\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equivalent\n",
      "        to the ``accuracy_score``. It differs in the multilabel classification\n",
      "        problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_\n",
      "    \n",
      "    log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of the true labels given a probabilistic classifier's\n",
      "        predictions. The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label yt in {0,1} and\n",
      "        estimated probability yp that yt = 1, the log loss is\n",
      "        \n",
      "            -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        eps : float\n",
      "            Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "            clipped to max(eps, min(1 - eps, p)).\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, optional (default=None)\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],  # doctest: +ELLIPSIS\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC)\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], default None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview\n",
      "           <https://doi.org/10.1093/bioinformatics/16.5.412>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)  # doctest: +ELLIPSIS\n",
      "        -0.33...\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            of shape (n_samples, n_outputs) or (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            of shape (n_samples, n_outputs) or (n_samples,)\n",
      "            Estimated targets as returned by a classifier\n",
      "        \n",
      "        sample_weight : array-like of shape = (n_samples,), optional\n",
      "            Sample weights\n",
      "        \n",
      "        labels : array-like\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data)\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : array, shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The multilabel_confusion_matrix calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while confusion_matrix calculates\n",
      "        one confusion matrix for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None)\n",
      "        Compute precision, recall, F-measure and support for each class\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        If ``pos_label is None`` and in binary classification, this function\n",
      "        returns the average precision, recall and F-measure if ``average``\n",
      "        is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, 1.0 by default\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None (default), 'binary', 'micro', 'macro', 'samples',                        'weighted']\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, , shape =        [n_unique_labels]\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        support : int (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        ... # doctest: +ELLIPSIS,+NORMALIZE_WHITESPACE\n",
      "        (array([0.        , 0.        , 0.66...]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined;\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, the metric will be set to 0, as will f-score, and\n",
      "        ``UndefinedMetricWarning`` will be raised.\n",
      "    \n",
      "    precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the precision\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)  # doctest: +ELLIPSIS\n",
      "        array([0.66..., 0.        , 0.        ])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``.\n",
      "    \n",
      "    recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
      "        Compute the recall\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : list, optional\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               parameter *labels* improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, 1 by default\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : string, [None, 'binary' (default), 'micro', 'macro', 'samples',                        'weighted']\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        precision_recall_fscore_support, balanced_accuracy_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``.\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, optional (default=True)\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape = [n_samples], optional\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, jaccard_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "FILE\n",
      "    d:\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
